{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Literal, Optional, Tuple\n",
    "\n",
    "import instructor\n",
    "import openai\n",
    "import pandas as pd\n",
    "import weave\n",
    "from pydantic import BaseModel, Field\n",
    "from set_env import set_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_env(\"OPENAI_API_KEY\")\n",
    "set_env(\"WANDB_API_KEY\")\n",
    "print(\"Env set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebooks.utils.config import ENTITY, WEAVE_PROJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weave.init(f\"{ENTITY}/{WEAVE_PROJECT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SAMPLES = 67"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebooks.utils.prompts import medical_system_prompt, medical_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.OpenAI()\n",
    "\n",
    "medical_dataset_url = \"https://raw.githubusercontent.com/wyim/aci-bench/main/data/challenge_data/train.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_medical_data(url: str, num_samples: int = N_SAMPLES) -> List[Dict]:\n",
    "    df = pd.read_csv(url)\n",
    "    print(df.shape)\n",
    "    samples = df.sample(n=num_samples, random_state=42)\n",
    "    return samples.to_dict(\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = load_medical_data(medical_dataset_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_transcript(record):\n",
    "    dialogue = record[\"dialogue\"].replace(\"\\n\", \" \")\n",
    "    note = record[\"note\"].replace(\"\\n\", \" \")\n",
    "    transcript = f\"Dialogue: {dialogue}\\n\\nMedical Note: {note}\"\n",
    "    return transcript\n",
    "\n",
    "\n",
    "@weave.op()\n",
    "def process_medical_record(record: Dict) -> Dict:\n",
    "    transcript = format_transcript(record)\n",
    "    prompt = medical_task.format(transcript=transcript)\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": medical_system_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    extracted_info = response.choices[0].message.content\n",
    "\n",
    "    return {\n",
    "        \"input\": transcript,\n",
    "        \"output\": extracted_info,\n",
    "    }\n",
    "\n",
    "\n",
    "@weave.op()\n",
    "def generate_medical_data(num_samples: int = N_SAMPLES) -> List[Dict]:\n",
    "    data = load_medical_data(medical_dataset_url, num_samples)\n",
    "    processed_data = []\n",
    "\n",
    "    for record in data:\n",
    "        processed_record = process_medical_record(record)\n",
    "        processed_data.append(processed_record)\n",
    "\n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = generate_medical_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weave.publish(results, name=\"medical_data_raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = instructor.patch(openai.OpenAI())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MainCriteria(BaseModel):\n",
    "    word_count: Literal[0, 1] = Field(\n",
    "        description=\"1 if the word count is within the limit of 150 words, 0 otherwise\",\n",
    "    )\n",
    "    presence_of_keys: Literal[0, 1] = Field(\n",
    "        description=\"1 if all the six targeted keys (Chief complaint, History of present illness, Physical examination, Symptoms, New medications with dosages, Follow-up instructions) are present, 0 otherwise\",\n",
    "    )\n",
    "    absence_of_PII: Literal[0, 1] = Field(\n",
    "        description=\"1 if no PII is present, 0 otherwise\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Make each desired field a separate annotation\n",
    "\n",
    "\n",
    "class AnnotationResult(BaseModel):\n",
    "    annotation: Literal[0, 1] = Field(\n",
    "        description=\"Binary score: 1 if the extraction meets all criteria, 0 if it fails on any\",\n",
    "    )\n",
    "    criteria_annotations: MainCriteria = Field(\n",
    "        description=\"A score for each of the main criteria\",\n",
    "    )\n",
    "    note: str = Field(\n",
    "        description=\"Brief explanation of the annotation decision, highlighting any issues or exemplary aspects\",\n",
    "    )\n",
    "\n",
    "\n",
    "annotation_prompt = \"\"\"\n",
    "    Review the following medical data extraction task results:\n",
    "\n",
    "    Task System Prompt:\n",
    "    {medical_system_prompt}\n",
    "\n",
    "    Task:\n",
    "    {medical_task}\n",
    "\n",
    "    Input:\n",
    "    {input_text}\n",
    "\n",
    "    Output:\n",
    "    {output_text}\n",
    "\n",
    "    Evaluate the extraction based on these criteria. Only refer to the Output in your evaluation and NOT the Medical Note field:\n",
    "    1. Completeness: All required fields addressed (Chief complaint, History of present illness, Physical examination, Symptoms, New medications with dosages, Follow-up instructions)\n",
    "    2. Accuracy: Information correctly extracted from input\n",
    "    3. Format: Proper bullet list format used (â€¢key: value)\n",
    "    4. Privacy: No personal identifiable information (PII) included\n",
    "    5. Conciseness: ~150 words, key information summarized\n",
    "    6. Use of \"N/A\" for missing information\n",
    "\n",
    "    Provide:\n",
    "    1. Annotation: 1 if the extraction meets all criteria, 0 if it fails on any\n",
    "    2. Note: Brief explanation of your decision, highlighting any issues or exemplary aspects\n",
    "\"\"\"\n",
    "\n",
    "annotation_system_prompt = \"\"\"\n",
    "You are an AI assistant tasked with evaluating medical data extraction results.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def process_annotation(input_text: str, output_text: str) -> AnnotationResult:\n",
    "    prompt = annotation_prompt.format(\n",
    "        medical_system_prompt=medical_system_prompt,\n",
    "        medical_task=medical_task,\n",
    "        input_text=input_text,\n",
    "        output_text=output_text,\n",
    "    )\n",
    "\n",
    "    return client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": annotation_system_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        response_model=AnnotationResult,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataPoint = Tuple[\n",
    "    dict,\n",
    "    dict,\n",
    "    Literal[0, 1],\n",
    "    MainCriteria,\n",
    "    str,\n",
    "    Optional[str],\n",
    "    Optional[str],\n",
    "]\n",
    "\n",
    "\n",
    "@weave.op()\n",
    "def generate_annotations(results: List[Dict]) -> List[DataPoint]:\n",
    "    annotations = []\n",
    "\n",
    "    for result in results:\n",
    "        input_text = result[\"input\"]\n",
    "        output_text = result[\"output\"]\n",
    "        annotation_result = process_annotation(input_text, output_text)\n",
    "\n",
    "        combined_task_description = (\n",
    "            f\"System Prompt: {medical_system_prompt}\\n\\nTask: {medical_task}\"\n",
    "        )\n",
    "\n",
    "        data_point: DataPoint = (\n",
    "            {\"input\": input_text},  # input\n",
    "            {\"output\": output_text},  # output\n",
    "            annotation_result.annotation,  # annotation (1 for correct, 0 for incorrect)\n",
    "            annotation_result.criteria_annotations.model_dump(),  # criteria_annotations\n",
    "            annotation_result.note,  # note\n",
    "            combined_task_description,  # human_description_for_task_or_judge\n",
    "            \"word count, presence of the six targeted keys, and absence of PII, with the first two implemented via code- based assertions and the last via an LLM evaluator\",  # human_description_for_metric_details\n",
    "        )\n",
    "\n",
    "        annotations.append(data_point)\n",
    "\n",
    "    return annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = generate_annotations(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weave.publish(annotations, name=\"medical_data_annotations\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
